{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INSTALLING DEPENDENCIES\n",
        "This cell installs all the necessary dependencies.\n"
      ],
      "metadata": {
        "id": "tz_DccEKDeM6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ar3HJpgO2OgG"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi uvicorn python-multipart\n",
        "!pip install google-generativeai groq\n",
        "!pip install PyMuPDF pdfplumber python-docx\n",
        "!pip install nest-asyncio pyngrok\n",
        "!apt-get install -y poppler-utils\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Colab deployment\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n"
      ],
      "metadata": {
        "id": "W2NVBJlgodl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing necessaary modules\n"
      ],
      "metadata": {
        "id": "niHSscPpD3Ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import tempfile\n",
        "from typing import Dict, List, Optional, Any\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "# FastAPI\n",
        "from fastapi import FastAPI, File, UploadFile, Form, HTTPException\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import uvicorn\n",
        "\n",
        "# Document Processing\n",
        "import fitz  # PyMuPDF\n",
        "import pdfplumber\n",
        "from docx import Document\n",
        "\n",
        "# AI APIs\n",
        "try:\n",
        "    import google.generativeai as genai\n",
        "    GEMINI_AVAILABLE = True\n",
        "except ImportError:\n",
        "    GEMINI_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from groq import Groq\n",
        "    GROQ_AVAILABLE = True\n",
        "except ImportError:\n",
        "    GROQ_AVAILABLE = False\n"
      ],
      "metadata": {
        "id": "k7rkGig_8DN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Environment Configuration\n",
        "MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB\n",
        "SUPPORTED_FORMATS = [\".pdf\", \".docx\", \".txt\"]\n",
        "\n",
        "# Global variables\n",
        "_global_api_key = None\n",
        "_current_provider = None  # 'gemini' or 'groq'\n"
      ],
      "metadata": {
        "id": "RBdpmVX38WXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SETTING API KEYS\n",
        "You can get the api keys free and insert it here (you can use gemini or groq)."
      ],
      "metadata": {
        "id": "1w4j35dZECoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_gemini_key(api_key: str):\n",
        "    \"\"\"\n",
        "    Set Google Gemini API key (FREE).\n",
        "\n",
        "    Get your free key at: https://makersuite.google.com/app/apikey\n",
        "\n",
        "    Args:\n",
        "        api_key: Your Gemini API key\n",
        "    \"\"\"\n",
        "    global _global_api_key, _current_provider\n",
        "    if not GEMINI_AVAILABLE:\n",
        "        print(\"❌ Google GenerativeAI not installed!\")\n",
        "        print(\"   Run: !pip install google-generativeai\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        genai.configure(api_key=api_key)\n",
        "        # Test the key\n",
        "        model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "        response = model.generate_content(\"test\")\n",
        "\n",
        "        _global_api_key = api_key\n",
        "        _current_provider = 'gemini'\n",
        "        os.environ['GEMINI_API_KEY'] = api_key\n",
        "        print(\"✅ Gemini API key set successfully!\")\n",
        "        print(\"🚀 Provider: Google Gemini (FREE)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Gemini API key verification failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def set_groq_key(api_key: str):\n",
        "    \"\"\"\n",
        "    Set Groq API key (FREE, very fast).\n",
        "\n",
        "    Get your free key at: https://console.groq.com/keys\n",
        "\n",
        "    Args:\n",
        "        api_key: Your Groq API key\n",
        "    \"\"\"\n",
        "    global _global_api_key, _current_provider\n",
        "    if not GROQ_AVAILABLE:\n",
        "        print(\"❌ Groq not installed!\")\n",
        "        print(\"   Run: !pip install groq\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        client = Groq(api_key=api_key)\n",
        "        # Test the key\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama-3.1-8b-instant\",\n",
        "            messages=[{\"role\": \"user\", \"content\": \"test\"}],\n",
        "            max_tokens=5\n",
        "        )\n",
        "\n",
        "        _global_api_key = api_key\n",
        "        _current_provider = 'groq'\n",
        "        os.environ['GROQ_API_KEY'] = api_key\n",
        "        print(\"✅ Groq API key set successfully!\")\n",
        "        print(\"🚀 Provider: Groq (FREE & Fast)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Groq API key verification failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def verify_api_setup():\n",
        "    \"\"\"Verify that an API key is properly configured.\"\"\"\n",
        "    if _current_provider is None:\n",
        "        print(\"❌ No API key configured!\")\n",
        "        print(\"\\n📝 Choose a FREE API provider:\")\n",
        "        print(\"\\n1. Google Gemini (RECOMMENDED - Completely FREE)\")\n",
        "        print(\"   • Get key: https://makersuite.google.com/app/apikey\")\n",
        "        print(\"   • Setup: set_gemini_key('your-key-here')\")\n",
        "        print(\"\\n2. Groq (FREE & Very Fast)\")\n",
        "        print(\"   • Get key: https://console.groq.com/keys\")\n",
        "        print(\"   • Setup: set_groq_key('your-key-here')\")\n",
        "        return False\n",
        "\n",
        "    print(f\"✅ API configured: {_current_provider.upper()}\")\n",
        "    return True\n",
        "\n"
      ],
      "metadata": {
        "id": "rCiXrXE68ez1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Resume Parsing Logic\n",
        "Handles extraction and parsing of resume content from various file formats."
      ],
      "metadata": {
        "id": "DA93dr8aEWB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResumeParser:\n",
        "    \"\"\"Handles extraction and parsing of resume content from various file formats.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_text_from_pdf(file_path: str) -> str:\n",
        "        \"\"\"Extract text from PDF using PyMuPDF with fallback to pdfplumber.\"\"\"\n",
        "        try:\n",
        "            text = \"\"\n",
        "            with fitz.open(file_path) as doc:\n",
        "                for page in doc:\n",
        "                    text += page.get_text()\n",
        "\n",
        "            if text.strip():\n",
        "                return text\n",
        "\n",
        "            logger.info(\"Falling back to pdfplumber for PDF extraction\")\n",
        "            with pdfplumber.open(file_path) as pdf:\n",
        "                text = \"\\n\".join([page.extract_text() or \"\" for page in pdf.pages])\n",
        "\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            logger.error(f\"PDF extraction error: {e}\")\n",
        "            raise ValueError(f\"Failed to extract text from PDF: {str(e)}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_text_from_docx(file_path: str) -> str:\n",
        "        \"\"\"Extract text from DOCX file.\"\"\"\n",
        "        try:\n",
        "            doc = Document(file_path)\n",
        "            text = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
        "\n",
        "            for table in doc.tables:\n",
        "                for row in table.rows:\n",
        "                    for cell in row.cells:\n",
        "                        text += \"\\n\" + cell.text\n",
        "\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            logger.error(f\"DOCX extraction error: {e}\")\n",
        "            raise ValueError(f\"Failed to extract text from DOCX: {str(e)}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_text_from_txt(file_path: str) -> str:\n",
        "        \"\"\"Extract text from TXT file with encoding detection.\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                return f.read()\n",
        "        except UnicodeDecodeError:\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='latin-1') as f:\n",
        "                    return f.read()\n",
        "            except Exception as e:\n",
        "                logger.error(f\"TXT extraction error: {e}\")\n",
        "                raise ValueError(f\"Failed to extract text from TXT: {str(e)}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_text(text: str) -> str:\n",
        "        \"\"\"Clean and preprocess extracted text.\"\"\"\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'[^\\w\\s.,;:()\\-@/+#]', '', text)\n",
        "        text = re.sub(r'\\n+', '\\n', text)\n",
        "        return text.strip()\n",
        "\n",
        "    @classmethod\n",
        "    def parse_resume(cls, file_path: str, file_extension: str) -> Dict[str, Any]:\n",
        "        \"\"\"Main parsing function that routes to appropriate extractor.\"\"\"\n",
        "        logger.info(f\"Parsing resume: {file_path} ({file_extension})\")\n",
        "\n",
        "        if file_extension == '.pdf':\n",
        "            raw_text = cls.extract_text_from_pdf(file_path)\n",
        "        elif file_extension == '.docx':\n",
        "            raw_text = cls.extract_text_from_docx(file_path)\n",
        "        elif file_extension == '.txt':\n",
        "            raw_text = cls.extract_text_from_txt(file_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file format: {file_extension}\")\n",
        "\n",
        "        if not raw_text or len(raw_text.strip()) < 50:\n",
        "            raise ValueError(\"Insufficient text extracted from resume.\")\n",
        "\n",
        "        cleaned_text = cls.clean_text(raw_text)\n",
        "\n",
        "        return {\n",
        "            \"raw_text\": raw_text,\n",
        "            \"cleaned_text\": cleaned_text,\n",
        "            \"file_type\": file_extension\n",
        "        }\n"
      ],
      "metadata": {
        "id": "7ncPgtt38p9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Resume Analyzing Logic\n",
        "Handles resume anlaysis using free API.\n"
      ],
      "metadata": {
        "id": "2QROtgySEnL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FreeAIResumeAnalyzer:\n",
        "    \"\"\"\n",
        "    Handles resume analysis using FREE AI APIs (Gemini or Groq).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, provider: str, api_key: str):\n",
        "        \"\"\"\n",
        "        Initialize the analyzer with chosen provider.\n",
        "\n",
        "        Args:\n",
        "            provider: 'gemini' or 'groq'\n",
        "            api_key: API key for the chosen provider\n",
        "        \"\"\"\n",
        "        self.provider = provider\n",
        "\n",
        "        if provider == 'gemini':\n",
        "            genai.configure(api_key=api_key)\n",
        "            self.model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "        elif provider == 'groq':\n",
        "            self.client = Groq(api_key=api_key)\n",
        "            self.model_name = \"llama-3.1-70b-versatile\"  # Best free model\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported provider: {provider}\")\n",
        "\n",
        "    def _generate_content(self, prompt: str, max_tokens: int = 2000) -> str:\n",
        "        \"\"\"Generate content using the configured provider.\"\"\"\n",
        "        try:\n",
        "            if self.provider == 'gemini':\n",
        "                response = self.model.generate_content(prompt)\n",
        "                return response.text\n",
        "\n",
        "            elif self.provider == 'groq':\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=self.model_name,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"You are an expert resume analyst and career coach.\"},\n",
        "                        {\"role\": \"user\", \"content\": prompt}\n",
        "                    ],\n",
        "                    temperature=0.4,\n",
        "                    max_tokens=max_tokens\n",
        "                )\n",
        "                return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Content generation error: {e}\")\n",
        "            raise\n",
        "\n",
        "    def extract_structured_info(self, resume_text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract structured information from resume.\"\"\"\n",
        "        prompt = f\"\"\"Extract structured information from this resume. Return ONLY valid JSON with these exact fields:\n",
        "{{\n",
        "    \"name\": \"full name or 'Not Found'\",\n",
        "    \"email\": \"email or 'Not Found'\",\n",
        "    \"phone\": \"phone or 'Not Found'\",\n",
        "    \"skills\": [\"skill1\", \"skill2\", ...],\n",
        "    \"experience\": [\n",
        "        {{\"title\": \"job title\", \"company\": \"company name\", \"duration\": \"time period\", \"description\": \"brief description\"}}\n",
        "    ],\n",
        "    \"education\": [\n",
        "        {{\"degree\": \"degree name\", \"institution\": \"school name\", \"year\": \"graduation year\"}}\n",
        "    ],\n",
        "    \"projects\": [\"project1\", \"project2\", ...]\n",
        "}}\n",
        "\n",
        "Resume:\n",
        "{resume_text[:4000]}\n",
        "\n",
        "Return ONLY the JSON, no other text.\"\"\"\n",
        "\n",
        "        try:\n",
        "            result = self._generate_content(prompt, max_tokens=1500)\n",
        "\n",
        "            # Clean response\n",
        "            if result.startswith(\"```\"):\n",
        "                result = re.sub(r'```json\\s*|\\s*```', '', result).strip()\n",
        "\n",
        "            return json.loads(result)\n",
        "        except json.JSONDecodeError as e:\n",
        "            logger.error(f\"JSON parsing error: {e}\")\n",
        "            return self._get_default_structure()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Extraction error: {e}\")\n",
        "            return self._get_default_structure()\n",
        "\n",
        "    def analyze_resume(self, resume_text: str, job_description: str,\n",
        "                      structured_info: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Comprehensive resume analysis.\"\"\"\n",
        "        prompt = f\"\"\"Analyze this resume against the job description. Provide a comprehensive evaluation.\n",
        "\n",
        "JOB DESCRIPTION:\n",
        "{job_description[:2000]}\n",
        "\n",
        "RESUME:\n",
        "{resume_text[:3000]}\n",
        "\n",
        "EXTRACTED INFO:\n",
        "Skills: {', '.join(structured_info.get('skills', [])[:20])}\n",
        "Experience: {len(structured_info.get('experience', []))} positions\n",
        "\n",
        "Provide analysis in this EXACT JSON format:\n",
        "{{\n",
        "    \"skills_analysis\": {{\n",
        "        \"score\": 0-100,\n",
        "        \"matching_skills\": [\"skill1\", \"skill2\"],\n",
        "        \"missing_skills\": [\"skill1\", \"skill2\"],\n",
        "        \"recommendations\": [\"specific suggestion 1\", \"specific suggestion 2\"]\n",
        "    }},\n",
        "    \"experience_analysis\": {{\n",
        "        \"score\": 0-100,\n",
        "        \"strengths\": [\"strength1\", \"strength2\"],\n",
        "        \"weaknesses\": [\"weakness1\", \"weakness2\"],\n",
        "        \"recommendations\": [\"specific suggestion 1\", \"specific suggestion 2\"]\n",
        "    }},\n",
        "    \"language_analysis\": {{\n",
        "        \"score\": 0-100,\n",
        "        \"grammar_quality\": \"excellent/good/fair/poor\",\n",
        "        \"readability\": \"excellent/good/fair/poor\",\n",
        "        \"professionalism\": \"excellent/good/fair/poor\",\n",
        "        \"recommendations\": [\"specific suggestion 1\", \"specific suggestion 2\"]\n",
        "    }},\n",
        "    \"formatting_analysis\": {{\n",
        "        \"score\": 0-100,\n",
        "        \"ats_friendly\": true/false,\n",
        "        \"structure_quality\": \"excellent/good/fair/poor\",\n",
        "        \"recommendations\": [\"specific suggestion 1\", \"specific suggestion 2\"]\n",
        "    }},\n",
        "    \"job_fit_score\": 0-100,\n",
        "    \"overall_recommendations\": [\"high-priority suggestion 1\", \"suggestion 2\", \"suggestion 3\"]\n",
        "}}\n",
        "\n",
        "Be specific, actionable, and honest. Return ONLY valid JSON.\"\"\"\n",
        "\n",
        "        try:\n",
        "            result = self._generate_content(prompt, max_tokens=2500)\n",
        "\n",
        "            # Clean response\n",
        "            if result.startswith(\"```\"):\n",
        "                result = re.sub(r'```json\\s*|\\s*```', '', result).strip()\n",
        "\n",
        "            analysis = json.loads(result)\n",
        "            return self._validate_analysis(analysis)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Analysis error: {e}\")\n",
        "            return self._get_default_analysis()\n",
        "\n",
        "    def generate_improved_resume(self, resume_text: str, job_description: str,\n",
        "                                analysis: Dict[str, Any]) -> str:\n",
        "        \"\"\"Generate an improved version of the resume.\"\"\"\n",
        "        recommendations = analysis.get('overall_recommendations', [])\n",
        "        missing_skills = analysis.get('skills_analysis', {}).get('missing_skills', [])\n",
        "\n",
        "        prompt = f\"\"\"Improve this resume based on the analysis and job requirements.\n",
        "\n",
        "ORIGINAL RESUME:\n",
        "{resume_text[:3000]}\n",
        "\n",
        "JOB DESCRIPTION:\n",
        "{job_description[:1500]}\n",
        "\n",
        "KEY RECOMMENDATIONS:\n",
        "{chr(10).join([f\"- {rec}\" for rec in recommendations[:5]])}\n",
        "\n",
        "MISSING SKILLS TO HIGHLIGHT:\n",
        "{', '.join(missing_skills[:10])}\n",
        "\n",
        "Create an improved version that:\n",
        "1. Enhances clarity and impact\n",
        "2. Adds quantifiable achievements\n",
        "3. Improves keyword optimization\n",
        "4. Maintains all original information\n",
        "5. Uses strong action verbs\n",
        "6. Improves formatting\n",
        "\n",
        "Return the improved resume in clean, professional format.\"\"\"\n",
        "\n",
        "        try:\n",
        "            return self._generate_content(prompt, max_tokens=3000)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Resume generation error: {e}\")\n",
        "            return \"Error generating improved resume. Please try again.\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_default_structure() -> Dict[str, Any]:\n",
        "        \"\"\"Return default structure when extraction fails.\"\"\"\n",
        "        return {\n",
        "            \"name\": \"Not Found\",\n",
        "            \"email\": \"Not Found\",\n",
        "            \"phone\": \"Not Found\",\n",
        "            \"skills\": [],\n",
        "            \"experience\": [],\n",
        "            \"education\": [],\n",
        "            \"projects\": []\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_default_analysis() -> Dict[str, Any]:\n",
        "        \"\"\"Return default analysis when analysis fails.\"\"\"\n",
        "        return {\n",
        "            \"skills_analysis\": {\"score\": 50, \"matching_skills\": [], \"missing_skills\": [], \"recommendations\": []},\n",
        "            \"experience_analysis\": {\"score\": 50, \"strengths\": [], \"weaknesses\": [], \"recommendations\": []},\n",
        "            \"language_analysis\": {\"score\": 50, \"grammar_quality\": \"unknown\", \"readability\": \"unknown\", \"professionalism\": \"unknown\", \"recommendations\": []},\n",
        "            \"formatting_analysis\": {\"score\": 50, \"ats_friendly\": False, \"structure_quality\": \"unknown\", \"recommendations\": []},\n",
        "            \"job_fit_score\": 50,\n",
        "            \"overall_recommendations\": [\"Analysis failed. Please try again.\"]\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def _validate_analysis(analysis: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Validate and sanitize analysis results.\"\"\"\n",
        "        for key in ['skills_analysis', 'experience_analysis', 'language_analysis', 'formatting_analysis']:\n",
        "            if key in analysis and 'score' in analysis[key]:\n",
        "                score = analysis[key]['score']\n",
        "                analysis[key]['score'] = max(0, min(100, int(score)))\n",
        "\n",
        "        if 'job_fit_score' in analysis:\n",
        "            analysis['job_fit_score'] = max(0, min(100, int(analysis['job_fit_score'])))\n",
        "\n",
        "        return analysis\n"
      ],
      "metadata": {
        "id": "ZQtTqzeo8ujI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scoring Engine\n",
        "It calculates and give scores according to the provided job description.\n"
      ],
      "metadata": {
        "id": "uVcoKRgwE3Lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScoringEngine:\n",
        "    \"\"\"Calculates final scores and compiles comprehensive results.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_overall_score(analysis: Dict[str, Any]) -> int:\n",
        "        \"\"\"Calculate weighted overall score from component scores.\"\"\"\n",
        "        weights = {\n",
        "            'skills_score': 0.30,\n",
        "            'experience_score': 0.25,\n",
        "            'language_score': 0.15,\n",
        "            'formatting_score': 0.10,\n",
        "            'job_fit_score': 0.20\n",
        "        }\n",
        "\n",
        "        skills_score = analysis.get('skills_analysis', {}).get('score', 50)\n",
        "        experience_score = analysis.get('experience_analysis', {}).get('score', 50)\n",
        "        language_score = analysis.get('language_analysis', {}).get('score', 50)\n",
        "        formatting_score = analysis.get('formatting_analysis', {}).get('score', 50)\n",
        "        job_fit_score = analysis.get('job_fit_score', 50)\n",
        "\n",
        "        overall = (\n",
        "            skills_score * weights['skills_score'] +\n",
        "            experience_score * weights['experience_score'] +\n",
        "            language_score * weights['language_score'] +\n",
        "            formatting_score * weights['formatting_score'] +\n",
        "            job_fit_score * weights['job_fit_score']\n",
        "        )\n",
        "\n",
        "        return int(round(overall))\n",
        "\n",
        "    @staticmethod\n",
        "    def compile_suggestions(analysis: Dict[str, Any]) -> List[str]:\n",
        "        \"\"\"Compile all suggestions into prioritized list.\"\"\"\n",
        "        suggestions = []\n",
        "        suggestions.extend(analysis.get('overall_recommendations', [])[:3])\n",
        "\n",
        "        for key in ['skills_analysis', 'experience_analysis', 'language_analysis', 'formatting_analysis']:\n",
        "            if key in analysis and 'recommendations' in analysis[key]:\n",
        "                suggestions.extend(analysis[key]['recommendations'][:2])\n",
        "\n",
        "        seen = set()\n",
        "        unique_suggestions = []\n",
        "        for suggestion in suggestions:\n",
        "            if suggestion.lower() not in seen:\n",
        "                seen.add(suggestion.lower())\n",
        "                unique_suggestions.append(suggestion)\n",
        "\n",
        "        return unique_suggestions[:10]\n",
        "\n",
        "    @classmethod\n",
        "    def generate_final_output(cls, analysis: Dict[str, Any],\n",
        "                            structured_info: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Generate final structured output for API response.\"\"\"\n",
        "        overall_score = cls.calculate_overall_score(analysis)\n",
        "        suggestions = cls.compile_suggestions(analysis)\n",
        "\n",
        "        return {\n",
        "            \"overall_score\": overall_score,\n",
        "            \"skills_score\": analysis.get('skills_analysis', {}).get('score', 50),\n",
        "            \"experience_score\": analysis.get('experience_analysis', {}).get('score', 50),\n",
        "            \"language_score\": analysis.get('language_analysis', {}).get('score', 50),\n",
        "            \"formatting_score\": analysis.get('formatting_analysis', {}).get('score', 50),\n",
        "            \"job_fit_score\": analysis.get('job_fit_score', 50),\n",
        "            \"suggestions\": suggestions,\n",
        "            \"detailed_analysis\": {\n",
        "                \"skills\": analysis.get('skills_analysis', {}),\n",
        "                \"experience\": analysis.get('experience_analysis', {}),\n",
        "                \"language\": analysis.get('language_analysis', {}),\n",
        "                \"formatting\": analysis.get('formatting_analysis', {})\n",
        "            },\n",
        "            \"extracted_info\": structured_info,\n",
        "            \"ai_provider\": _current_provider\n",
        "        }\n"
      ],
      "metadata": {
        "id": "Y4k_VwNv80yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FAST API\n",
        "Provides an ui for the user."
      ],
      "metadata": {
        "id": "2pOEPACjFJOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app = FastAPI(\n",
        "    title=\"AI Resume Analyzer (FREE APIs)\",\n",
        "    description=\"Resume analysis using FREE AI APIs (Gemini/Groq)\",\n",
        "    version=\"2.0.0\"\n",
        ")\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "def get_analyzer():\n",
        "    \"\"\"Get or create the AI analyzer instance.\"\"\"\n",
        "    if _current_provider is None or _global_api_key is None:\n",
        "        raise HTTPException(\n",
        "            status_code=500,\n",
        "            detail=\"API not configured. Please set up Gemini or Groq API key first.\"\n",
        "        )\n",
        "    return FreeAIResumeAnalyzer(_current_provider, _global_api_key)\n",
        "\n",
        "@app.post(\"/analyze_resume/\")\n",
        "async def analyze_resume(\n",
        "    resume: UploadFile = File(...),\n",
        "    job_description: str = Form(...),\n",
        "    generate_improved: bool = Form(False)\n",
        "):\n",
        "    \"\"\"Analyze a resume against a job description using FREE AI APIs.\"\"\"\n",
        "    temp_file = None\n",
        "\n",
        "    try:\n",
        "        analyzer = get_analyzer()\n",
        "\n",
        "        if not resume.filename:\n",
        "            raise HTTPException(status_code=400, detail=\"No file provided\")\n",
        "\n",
        "        file_extension = Path(resume.filename).suffix.lower()\n",
        "        if file_extension not in SUPPORTED_FORMATS:\n",
        "            raise HTTPException(\n",
        "                status_code=400,\n",
        "                detail=f\"Unsupported format. Supported: {', '.join(SUPPORTED_FORMATS)}\"\n",
        "            )\n",
        "\n",
        "        if not job_description or len(job_description.strip()) < 20:\n",
        "            raise HTTPException(\n",
        "                status_code=400,\n",
        "                detail=\"Job description must be at least 20 characters\"\n",
        "            )\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as temp_file:\n",
        "            content = await resume.read()\n",
        "\n",
        "            if len(content) > MAX_FILE_SIZE:\n",
        "                raise HTTPException(\n",
        "                    status_code=400,\n",
        "                    detail=f\"File too large. Max: {MAX_FILE_SIZE / 1024 / 1024}MB\"\n",
        "                )\n",
        "\n",
        "            temp_file.write(content)\n",
        "            temp_file_path = temp_file.name\n",
        "\n",
        "        logger.info(\"Parsing resume...\")\n",
        "        parsed_data = ResumeParser.parse_resume(temp_file_path, file_extension)\n",
        "\n",
        "        logger.info(\"Extracting structured information...\")\n",
        "        structured_info = analyzer.extract_structured_info(parsed_data['cleaned_text'])\n",
        "\n",
        "        logger.info(\"Analyzing resume with AI...\")\n",
        "        analysis = analyzer.analyze_resume(\n",
        "            parsed_data['cleaned_text'],\n",
        "            job_description,\n",
        "            structured_info\n",
        "        )\n",
        "\n",
        "        logger.info(\"Compiling results...\")\n",
        "        final_output = ScoringEngine.generate_final_output(analysis, structured_info)\n",
        "\n",
        "        if generate_improved:\n",
        "            logger.info(\"Generating improved resume...\")\n",
        "            improved_resume = analyzer.generate_improved_resume(\n",
        "                parsed_data['raw_text'],\n",
        "                job_description,\n",
        "                analysis\n",
        "            )\n",
        "            final_output['improved_resume'] = improved_resume\n",
        "\n",
        "        return JSONResponse(content=final_output)\n",
        "\n",
        "    except ValueError as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Internal error: {str(e)}\")\n",
        "    finally:\n",
        "        if temp_file and os.path.exists(temp_file.name):\n",
        "            try:\n",
        "                os.unlink(temp_file.name)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Failed to delete temp file: {e}\")\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    \"\"\"Health check endpoint.\"\"\"\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"version\": \"2.0.0\",\n",
        "        \"provider\": _current_provider or \"not_configured\",\n",
        "        \"free_api\": True\n",
        "    }\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    \"\"\"Root endpoint with API information.\"\"\"\n",
        "    return {\n",
        "        \"message\": \"AI Resume Analyzer API (FREE)\",\n",
        "        \"version\": \"2.0.0\",\n",
        "        \"provider\": _current_provider or \"not_configured\",\n",
        "        \"endpoints\": {\n",
        "            \"/analyze_resume/\": \"POST - Analyze resume\",\n",
        "            \"/health\": \"GET - Health check\",\n",
        "            \"/docs\": \"GET - API documentation\"\n",
        "        }\n",
        "    }\n"
      ],
      "metadata": {
        "id": "myqtWgJN85C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running on server using ngrok\n",
        "Running fastapi in colab or jupyter notebook without interfering in running other cells operation."
      ],
      "metadata": {
        "id": "IEsvz6yLFV8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "\n",
        "def run_colab_server(port: int = 8001, use_ngrok: bool = False, ngrok_token: str = None):\n",
        "    \"\"\"Run FastAPI server in Colab without blocking other cells.\"\"\"\n",
        "    nest_asyncio.apply()\n",
        "\n",
        "    if use_ngrok:\n",
        "        from pyngrok import ngrok\n",
        "        if ngrok_token:\n",
        "            ngrok.set_auth_token(ngrok_token)\n",
        "        public_url = ngrok.connect(port)\n",
        "        print(f\"📡 Public URL: {public_url}\")\n",
        "        print(f\"📚 API Docs: {public_url}/docs\")\n",
        "    else:\n",
        "        print(f\"🚀 Running locally on port {port} (Colab VM)\")\n",
        "        print(f\"📚 Docs: http://localhost:{port}/docs\")\n",
        "\n",
        "    # Run server in background thread\n",
        "    def start():\n",
        "        uvicorn.run(app, host=\"0.0.0.0\", port=port, log_level=\"info\")\n",
        "\n",
        "    thread = threading.Thread(target=start, daemon=True)\n",
        "    thread.start()\n"
      ],
      "metadata": {
        "id": "ZnOmRxXy88c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "    except ImportError:\n",
        "        IN_COLAB = False\n",
        "\n",
        "    if IN_COLAB:\n",
        "        print(\"🔬 Running in Google Colab mode\")\n",
        "        print(\"\\n📝 Setup Instructions:\")\n",
        "        print(\"1. Get FREE API key:\")\n",
        "        print(\"   • Gemini: https://makersuite.google.com/app/apikey\")\n",
        "        print(\"   • Groq: https://console.groq.com/keys\")\n",
        "        print(\"\\n2. Set your key:\")\n",
        "        print(\"   set_gemini_key('your-key-here')  # OR\")\n",
        "        print(\"   set_groq_key('your-key-here')\")\n",
        "        print(\"\\n3. Start server:\")\n",
        "        print(\"   run_colab_server()\")\n",
        "    else:\n",
        "        print(\"💻 Running in local mode\")\n",
        "        if not verify_api_setup():\n",
        "            print(\"\\nPlease set up an API key first!\")\n",
        "        else:\n",
        "            uvicorn.run(app, host=\"0.0.0.0\", port=8000, reload=True)\n"
      ],
      "metadata": {
        "id": "jCrWGH219AwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gemini API key\n",
        "Provide the api key that you have copied from google ai studio or groq api ."
      ],
      "metadata": {
        "id": "ieYyHaImFrYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your actual\n",
        "set_gemini_key('Your_gemini_api_key')"
      ],
      "metadata": {
        "id": "eKcUd_aVlKPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking the API"
      ],
      "metadata": {
        "id": "ciJSGHlkF5iL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Check if everything is ready\n",
        "if verify_api_setup():\n",
        "    print(\"🎉 Ready to analyze resumes!\")\n",
        "else:\n",
        "    print(\"⚠️ Please set an API key first\")\n"
      ],
      "metadata": {
        "id": "c0rIz84tlP8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Killing all the previous running servers\n",
        "!killall ngrok"
      ],
      "metadata": {
        "id": "FWGG6VSgnJ8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Running** the server using ngrok"
      ],
      "metadata": {
        "id": "xmKJMZ_rGIFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_colab_server(port=8000, use_ngrok=True)"
      ],
      "metadata": {
        "id": "2lZZHW3zsrHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking the resume against the job description provided and giving details,scores and suggestions in an json file."
      ],
      "metadata": {
        "id": "ZRDVEWEjGQRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "# Upload your resume\n",
        "print(\"📄 Upload your resume (PDF, DOCX, or TXT):\")\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# Replace with YOUR ngrok URL from Step 6\n",
        "API_URL = \"https://1cff299cd342.ngrok-free.app\"\n",
        "\n",
        "# Prepare the request\n",
        "url = f\"{API_URL}/analyze_resume/\"\n",
        "files_data = {'resume': open(filename, 'rb')}\n",
        "form_data = {\n",
        "    'job_description': '''\n",
        "    Senior Software Engineer\n",
        "\n",
        "    We're looking for an experienced software engineer to join our team.\n",
        "\n",
        "    Required Skills:\n",
        "    - 5+ years of software development experience\n",
        "    - Strong programming skills (Python, Java, or JavaScript)\n",
        "    - Experience with web frameworks (Django, Flask, React, Angular)\n",
        "    - Database knowledge (SQL, NoSQL)\n",
        "    - RESTful API design\n",
        "    - Git version control\n",
        "    - Agile methodology\n",
        "\n",
        "    Preferred Skills:\n",
        "    - Cloud platforms (AWS, GCP, Azure)\n",
        "    - Docker and Kubernetes\n",
        "    - CI/CD pipelines\n",
        "    - Machine Learning basics\n",
        "    - Microservices architecture\n",
        "\n",
        "    Responsibilities:\n",
        "    - Design and develop scalable applications\n",
        "    - Write clean, maintainable code\n",
        "    - Collaborate with cross-functional teams\n",
        "    - Mentor junior developers\n",
        "    - Participate in code reviews\n",
        "    ''',\n",
        "    'generate_improved': 'True'  # Set to 'true' for improved resume\n",
        "}\n",
        "\n",
        "# Analyze the resume\n",
        "print(\"\\n⏳ Analyzing your resume with AI...\")\n",
        "response = requests.post(url, files=files_data, data=form_data)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "\n",
        "    # Display beautiful results\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"📊 RESUME ANALYSIS RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\n🎯 Overall Score: {result['overall_score']}/100\")\n",
        "    print(f\"💡 Skills Score: {result['skills_score']}/100\")\n",
        "    print(f\"💼 Experience Score: {result['experience_score']}/100\")\n",
        "    print(f\"✍️  Language Score: {result['language_score']}/100\")\n",
        "    print(f\"📋 Formatting Score: {result['formatting_score']}/100\")\n",
        "    print(f\"🎪 Job Fit Score: {result['job_fit_score']}/100\")\n",
        "    print(f\"🤖 AI Provider: {result['ai_provider'].upper()}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"💡 TOP SUGGESTIONS FOR IMPROVEMENT:\")\n",
        "    print(\"=\"*70)\n",
        "    for i, suggestion in enumerate(result['suggestions'], 1):\n",
        "        print(f\"\\n{i}. {suggestion}\")\n",
        "\n",
        "    # Show detailed analysis\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"🔍 DETAILED SKILLS ANALYSIS:\")\n",
        "    print(\"=\"*70)\n",
        "    skills = result['detailed_analysis']['skills']\n",
        "    print(f\"\\n✅ Matching Skills: {', '.join(skills.get('matching_skills', [])[:5])}\")\n",
        "    print(f\"❌ Missing Skills: {', '.join(skills.get('missing_skills', [])[:5])}\")\n",
        "\n",
        "    # Save results\n",
        "    with open('resume_analysis.json', 'w') as f:\n",
        "        json.dump(result, f, indent=2)\n",
        "    print(\"\\n\\n✅ Full results saved to 'resume_analysis.json'\")\n",
        "\n",
        "    # Download results\n",
        "    files.download('resume_analysis.json')\n",
        "\n",
        "else:\n",
        "    print(f\"❌ Error: {response.status_code}\")\n",
        "    print(response.text)\n",
        "\n"
      ],
      "metadata": {
        "id": "nTLfqS-9nSRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A feature for the one who wants to improve thier resume for a particular job domain and also improving the quality of the resume."
      ],
      "metadata": {
        "id": "AZhjJPfdGkKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "# Upload your resume\n",
        "print(\"📄 Upload your resume (PDF, DOCX, or TXT):\")\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# Replace with YOUR ngrok URL from Step 6\n",
        "API_URL = \"https://1cff299cd342.ngrok-free.app\"\n",
        "\n",
        "# Prepare the request\n",
        "url = f\"{API_URL}/analyze_resume/\"\n",
        "files_data = {'resume': open(filename, 'rb')}\n",
        "form_data = {\n",
        "    'job_description': '''\n",
        "    Senior Software Engineer\n",
        "\n",
        "    We're looking for an experienced software engineer to join our team.\n",
        "\n",
        "    Required Skills:\n",
        "    - 5+ years of software development experience\n",
        "    - Strong programming skills (Python, Java, or JavaScript)\n",
        "    - Experience with web frameworks (Django, Flask, React, Angular)\n",
        "    - Database knowledge (SQL, NoSQL)\n",
        "    - RESTful API design\n",
        "    - Git version control\n",
        "    - Agile methodology\n",
        "\n",
        "    Preferred Skills:\n",
        "    - Cloud platforms (AWS, GCP, Azure)\n",
        "    - Docker and Kubernetes\n",
        "    - CI/CD pipelines\n",
        "    - Machine Learning basics\n",
        "    - Microservices architecture\n",
        "\n",
        "    Responsibilities:\n",
        "    - Design and develop scalable applications\n",
        "    - Write clean, maintainable code\n",
        "    - Collaborate with cross-functional teams\n",
        "    - Mentor junior developers\n",
        "    - Participate in code reviews\n",
        "    ''',\n",
        "    'generate_improved': 'True'  # Set to 'true' for improved resume\n",
        "}\n",
        "\n",
        "# After getting results:\n",
        "if 'improved_resume' in result:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"✨ IMPROVED RESUME VERSION:\")\n",
        "    print(\"=\"*70)\n",
        "    print(result['improved_resume'])\n",
        "\n",
        "    # Save improved resume\n",
        "    with open('improved_resume.txt', 'w') as f:\n",
        "        f.write(result['improved_resume'])\n",
        "    print(\"\\n✅ Improved resume saved to 'improved_resume.txt'\")\n",
        "    files.download('improved_resume.txt')\n"
      ],
      "metadata": {
        "id": "IV0Picuc6xD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IsCbySoi8K6w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}